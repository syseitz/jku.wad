{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZLFJuX3_vID"
   },
   "source": [
    "# ðŸ§  Deep Reinforcement Learning â€” Doom Agent (SS2025)\n",
    "\n",
    "Welcome to the last assignment for the **Deep Reinforcement Learning** course (SS2025). In this notebook, you'll implement and train a reinforcement learning agent to play **Doom**.\n",
    "\n",
    "You will:\n",
    "- Set up a custom VizDoom environment with shaped rewards\n",
    "- Train an agent using an approach of your choice\n",
    "- Track reward components across episodes\n",
    "- Evaluate the best model\n",
    "- Visualize performance with replays and GIFs\n",
    "- Export the trained agent to ONNX to submit to the evaluation server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mi5FBK3oi1lI",
    "outputId": "6a38df99-16cd-4a65-8bb0-ead70efcbd71"
    },
   "outputs": [],
   "source": [
    "# Install the dependencies\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install --upgrade notebook ipywidgets ipykernel -q\n",
    "!pip install torch numpy matplotlib vizdoom portpicker gym onnx wandb stable-baselines3 stable-baselines3[extra] Shimmy einops torchvision -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYTcQU_TiP06"
    },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "base_dir = os.path.abspath(os.getcwd())\n",
    "dir_path = os.path.join(base_dir, \"jku.wad\")\n",
    "\n",
    "if os.path.isdir(dir_path):\n",
    "    os.chdir(dir_path)\n",
    "    subprocess.run([\"git\", \"pull\", \"origin\", \"main\"])\n",
    "else:\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/syseitz/jku.wad.git\", dir_path])\n",
    "    os.chdir(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDz9CHjVzP61"
   },
   "source": [
    "## Environment configuration\n",
    "\n",
    "ViZDoom supports multiple visual buffers that can be used as input for training agents. Each buffer provides different information about the game environment, as seen from left to right:\n",
    "\n",
    "Screen\n",
    "- The default first-person RGB view seen by the agent.\n",
    "\n",
    "Labels\n",
    "- A semantic map where each pixel is tagged with an object ID (e.g., enemy, item, wall).\n",
    "\n",
    "Depth\n",
    "- A grayscale map showing the distance from the agent to surfaces in the scene.\n",
    "\n",
    "Automap\n",
    "- A top-down schematic view of the map, useful for global navigation tasks.\n",
    "\n",
    "![buffers gif](https://vizdoom.farama.org/_images/vizdoom-demo.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmNDlnmfzP62"
    },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from typing import Dict, Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "from collections import deque, OrderedDict\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from gym import Env\n",
    "import gymnasium as gym\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "\n",
    "from doom_arena import VizdoomMPEnv\n",
    "from doom_arena.reward import VizDoomReward\n",
    "from doom_arena.render import render_episode\n",
    "from IPython.display import HTML\n",
    "\n",
    "from vizdoom import ScreenFormat\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "USE_GRAYSCALE = False  # â† flip to False for RGB\n",
    "\n",
    "PLAYER_CONFIG = {\n",
    "    \"n_stack_frames\": 1,\n",
    "    \"extra_state\": [\"depth\", \"labels\"],\n",
    "    \"hud\": \"none\",\n",
    "    \"crosshair\": True,\n",
    "    \"screen_format\": ScreenFormat.GRAY8 if USE_GRAYSCALE else ScreenFormat.CRCGCB,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLoMT0EMzP63"
   },
   "source": [
    "## Reward function\n",
    "\n",
    "In this task, you will define a reward function to guide the agent's learning. The function is called at every step and receives the current and previous game variables (e.g., number of frags, hits taken, health).\n",
    "\n",
    "Your goal is to combine these into a meaningful reward, encouraging desirable behavior, such as:\n",
    "\n",
    "- Rewarding frags (enemy kills)\n",
    "\n",
    "- Rewarding accuracy (hitting enemies)\n",
    "\n",
    "- Penalizing damage taken\n",
    "\n",
    "- (Optional) Encouraging survival, ammo efficiency, etc.\n",
    "\n",
    "You can return multiple reward components, which are summed during training. Consider the class below as a great starting point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gq1xlneLzP63"
   },
   "outputs": [],
   "source": [
    "# TODO: environment training parameters\n",
    "EPISODE_TIMEOUT = 1100\n",
    "NUM_BOTS = 4\n",
    "# TODO: model hyperparameters\n",
    "GAMMA = 0.95\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.5\n",
    "EXPLORATION_FRACTION = 1.0\n",
    "FEATURES_DIM = 512\n",
    "TOTAL_TIMESTEPS = 1000000\n",
    "\n",
    "class YourReward(VizDoomReward):\n",
    "    def __init__(self, num_players: int):\n",
    "        super().__init__(num_players)\n",
    "        self.last_rewards = [None] * num_players\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        vizdoom_reward: float,\n",
    "        game_var: Dict[str, float],\n",
    "        game_var_old: Dict[str, float],\n",
    "        player_id: int,\n",
    "    ) -> Tuple[float, ...]:\n",
    "        self._step += 1\n",
    "        _ = vizdoom_reward, player_id\n",
    "\n",
    "        damage_done = game_var[\"DAMAGECOUNT\"] - game_var_old[\"DAMAGECOUNT\"]\n",
    "        rwd_damage = 0.01 * damage_done\n",
    "\n",
    "        rwd_frag = 1.0 * (game_var[\"FRAGCOUNT\"] - game_var_old[\"FRAGCOUNT\"])\n",
    "\n",
    "        ammo_delta = game_var_old[\"SELECTED_WEAPON_AMMO\"] - game_var[\"SELECTED_WEAPON_AMMO\"]\n",
    "        if ammo_delta > 0:\n",
    "            shots_fired = ammo_delta\n",
    "            hits = game_var[\"HITCOUNT\"] - game_var_old[\"HITCOUNT\"]\n",
    "            missed_shots = max(0, shots_fired - hits)\n",
    "            rwd_missed = -0.1 * missed_shots\n",
    "        else:\n",
    "            rwd_missed = 0\n",
    "\n",
    "        rwd_survival = 0.001\n",
    "        rwd_dead = -0.5 if game_var[\"DEAD\"] == 1 else 0.0\n",
    "\n",
    "        rwd_spam_penalty = -0.01 if ammo_delta > 0 and damage_done <= 0 else 0.0\n",
    "\n",
    "        health_delta = game_var[\"HEALTH\"] - game_var_old[\"HEALTH\"]\n",
    "        health_gained = max(0, health_delta)\n",
    "        rwd_health_pickup = 0.02 * health_gained\n",
    "\n",
    "        position_changed = (game_var[\"POSITION_X\"] != game_var_old[\"POSITION_X\"]) or (game_var[\"POSITION_Y\"] != game_var_old[\"POSITION_Y\"])\n",
    "        rwd_movement = 0.00005 if position_changed else -0.0025\n",
    "\n",
    "        rewards = (rwd_damage, rwd_frag, rwd_missed, rwd_survival, rwd_dead, rwd_spam_penalty, rwd_health_pickup, rwd_movement)\n",
    "        self.last_rewards[player_id] = rewards\n",
    "        \n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzTS4m2VzP63"
    },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "DTYPE = torch.float32\n",
    "\n",
    "reward_fn = YourReward(num_players=1)\n",
    "\n",
    "env = VizdoomMPEnv(\n",
    "    num_players=1,\n",
    "    num_bots=NUM_BOTS,\n",
    "    bot_skill=0,\n",
    "    doom_map=\"ROOM\",\n",
    "    extra_state=PLAYER_CONFIG[\"extra_state\"],\n",
    "    episode_timeout=EPISODE_TIMEOUT,\n",
    "    n_stack_frames=PLAYER_CONFIG[\"n_stack_frames\"],\n",
    "    crosshair=PLAYER_CONFIG[\"crosshair\"],\n",
    "    hud=PLAYER_CONFIG[\"hud\"],\n",
    "    reward_fn=reward_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkuIefaSzP64"
   },
   "source": [
    "## Agent\n",
    "\n",
    "Implement your agent using Stable Baselines3's DQN with the default CnnPolicy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7dWjwTdzP65"
    },
   "outputs": [],
   "source": [
    "model = DQN(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    buffer_size=REPLAY_BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    gamma=GAMMA,\n",
    "    exploration_fraction=EXPLORATION_FRACTION,\n",
    "    exploration_initial_eps=EPSILON_START,\n",
    "    exploration_final_eps=EPSILON_END,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5V4XMeIk5Uv"
    },
   "outputs": [],
   "source": [
    "class EpisodeCallback(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super(EpisodeCallback, self).__init__()\n",
    "        self.episode_reward = 0\n",
    "        self.episode_num = 0\n",
    "        self.episode_rwd_components = {\n",
    "            \"damage\": 0.0,\n",
    "            \"frag\": 0.0,\n",
    "            \"missed\": 0.0,\n",
    "            \"survival\": 0.0,\n",
    "            \"dead\": 0.0,\n",
    "            \"spam_penalty\": 0.0,\n",
    "            \"health\": 0.0,\n",
    "            \"movement\": 0.0\n",
    "        }\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.episode_reward += self.locals['rewards'][0]\n",
    "\n",
    "        monitor_env = self.locals['env'].envs[0]\n",
    "        actual_env = monitor_env.env\n",
    "        last_rewards = actual_env.reward_fn.last_rewards[0]\n",
    "\n",
    "        if last_rewards is not None:\n",
    "            self.episode_rwd_components[\"damage\"] += last_rewards[0]\n",
    "            self.episode_rwd_components[\"frag\"] += last_rewards[1]\n",
    "            self.episode_rwd_components[\"missed\"] += last_rewards[2]\n",
    "            self.episode_rwd_components[\"survival\"] += last_rewards[3]\n",
    "            self.episode_rwd_components[\"dead\"] += last_rewards[4]\n",
    "            self.episode_rwd_components[\"spam_penalty\"] += last_rewards[5]\n",
    "            self.episode_rwd_components[\"health\"] += last_rewards[6]\n",
    "            self.episode_rwd_components[\"movement\"] += last_rewards[7]\n",
    "\n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_num += 1\n",
    "            wandb.log({\n",
    "                \"episode\": self.episode_num,\n",
    "                \"return\": self.episode_reward,\n",
    "                \"rwd_damage\": self.episode_rwd_components[\"damage\"],\n",
    "                \"rwd_frag\": self.episode_rwd_components[\"frag\"],\n",
    "                \"rwd_missed\": self.episode_rwd_components[\"missed\"],\n",
    "                \"rwd_survival\": self.episode_rwd_components[\"survival\"],\n",
    "                \"rwd_dead\": self.episode_rwd_components[\"dead\"],\n",
    "                \"rwd_spam_penalty\": self.episode_rwd_components[\"spam_penalty\"],\n",
    "                \"rwd_health\": self.episode_rwd_components[\"health\"],\n",
    "                \"rwd_movement\": self.episode_rwd_components[\"movement\"],\n",
    "            })\n",
    "            self.episode_reward = 0\n",
    "            for key in self.episode_rwd_components:\n",
    "                self.episode_rwd_components[key] = 0.0\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mihj8Q5xzP65"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5V4XMeIk5Uv"
    },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"doom-rl\", entity=\"soerenseitz-university-of-vienna\", config={\n",
    "    \"gamma\": GAMMA,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"replay_buffer_size\": REPLAY_BUFFER_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epsilon_start\": EPSILON_START,\n",
    "    \"epsilon_end\": EPSILON_END,\n",
    "    \"exploration_fraction\": EXPLORATION_FRACTION,\n",
    "    \"num_bots\": NUM_BOTS,\n",
    "    \"episode_timeout\": EPISODE_TIMEOUT,\n",
    "    \"use_grayscale\": USE_GRAYSCALE,\n",
    "    \"extra_state\": PLAYER_CONFIG[\"extra_state\"],\n",
    "    \"hud\": PLAYER_CONFIG[\"hud\"],\n",
    "    \"crosshair\": PLAYER_CONFIG[\"crosshair\"],\n",
    "    \"screen_format\": PLAYER_CONFIG[\"screen_format\"].name,\n",
    "    \"doom_map\": \"ROOM\",\n",
    "})\n",
    "callback = EpisodeCallback()\n",
    "\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callback, progress_bar=True)\n",
    "final_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpBx9O9yzP66"
   },
   "source": [
    "## Dump to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNkgBbUSzP66"
    },
   "outputs": [],
   "source": [
    "import onnx\n",
    "import json\n",
    "import torch\n",
    "\n",
    "def onnx_dump(env, model, config, run, filename_prefix=\"model\"):\n",
    "    dummy_input = torch.randn(1, *env.observation_space.shape).float().to('cpu')\n",
    "    print(\"Dummy input shape:\", dummy_input.shape)\n",
    "    \n",
    "    policy_net = model.policy.to('cpu')\n",
    "    \n",
    "    run_id = run.id\n",
    "    filename = f\"{filename_prefix}_{run_id}.onnx\"\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        policy_net,\n",
    "        args=dummy_input,\n",
    "        f=filename,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    )\n",
    "    \n",
    "    onnx_model = onnx.load(filename)\n",
    "    meta = onnx_model.metadata_props.add()\n",
    "    meta.key = \"config\"\n",
    "    meta.value = json.dumps(config)\n",
    "    onnx.save(onnx_model, filename)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "export_config = {\n",
    "    **{k: str(v) if isinstance(v, ScreenFormat) else v for k, v in PLAYER_CONFIG.items()},\n",
    "    \"algo_type\": \"Q\",\n",
    "}\n",
    "\n",
    "filename = onnx_dump(env, final_model, export_config, run, filename_prefix=\"model\")\n",
    "print(f\"Best network exported to {filename}\")\n",
    "\n",
    "artifact = wandb.Artifact('model', type='model')\n",
    "artifact.add_file(filename)\n",
    "run.log_artifact(artifact)\n",
    "artifact.wait()\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rZCsfQHFMu-"
   },
   "source": [
    "## Evaluation and Visualization\n",
    "\n",
    "In this final section, you can evaluate your trained agent, inspect its performance visually, and analyze reward components over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARC2K2k686nu"
    },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_reward_components(reward_log, smooth_window: int = 5):\n",
    "    if not reward_log:\n",
    "        print(\"reward_log is empty â€“ nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(reward_log)\n",
    "    df_smooth = df.rolling(window=smooth_window, min_periods=1).mean()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for col in df.columns:\n",
    "        plt.plot(df.index, df[col], label=col)\n",
    "    plt.title(\"Raw episode reward components\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for col in df.columns:\n",
    "        plt.plot(df.index, df_smooth[col], label=f\"{col} (avg)\")\n",
    "    plt.title(f\"Smoothed (window={smooth_window})\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Hint for replay visualisation:\n",
    "# env.enable_replay()\n",
    "# ... run an evaluation episode ...\n",
    "# env.disable_replay()\n",
    "# replays = env.get_player_replays()\n",
    "# from doom_arena.render import render_episode\n",
    "# from IPython.display import HTML\n",
    "# HTML(render_episode(replays, subsample=5).to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mhd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}