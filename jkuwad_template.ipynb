{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZLFJuX3_vID"
   },
   "source": [
    "# 🧠 Deep Reinforcement Learning — Doom Agent (SS2025)\n",
    "\n",
    "Welcome to the last assignment for the **Deep Reinforcement Learning** course (SS2025). In this notebook, you'll implement and train a reinforcement learning agent to play **Doom**.\n",
    "\n",
    "You will:\n",
    "- Set up a custom VizDoom environment with shaped rewards\n",
    "- Train an agent using an approach of your choice\n",
    "- Track reward components across episodes\n",
    "- Evaluate the best model\n",
    "- Visualize performance with replays and GIFs\n",
    "- Export the trained agent to ONNX to submit to the evaluation server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oYTcQU_TiP06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (25.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install the dependencies\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install --upgrade notebook ipywidgets ipykernel -q\n",
    "!pip install torch numpy matplotlib vizdoom portpicker gym onnx wandb stable-baselines3 stable-baselines3[extra] Shimmy einops torchvision -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mi5FBK3oi1lI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/jku.wad'...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "base_dir = os.path.abspath(os.getcwd())\n",
    "dir_path = os.path.join(base_dir, \"jku.wad\")\n",
    "\n",
    "if os.path.isdir(dir_path):\n",
    "    os.chdir(dir_path)\n",
    "    subprocess.run([\"git\", \"pull\", \"origin\", \"main\"])\n",
    "else:\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/syseitz/jku.wad.git\", dir_path])\n",
    "    os.chdir(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDz9CHjVzP61"
   },
   "source": [
    "## Environment configuration\n",
    "\n",
    "ViZDoom supports multiple visual buffers that can be used as input for training agents. Each buffer provides different information about the game environment, as seen from left to right:\n",
    "\n",
    "\n",
    "Screen\n",
    "- The default first-person RGB view seen by the agent.\n",
    "\n",
    "Labels\n",
    "- A semantic map where each pixel is tagged with an object ID (e.g., enemy, item, wall).\n",
    "\n",
    "Depth\n",
    "- A grayscale map showing the distance from the agent to surfaces in the scene.\n",
    "\n",
    "Automap\n",
    "- A top-down schematic view of the map, useful for global navigation tasks.\n",
    "\n",
    "![buffers gif](https://vizdoom.farama.org/_images/vizdoom-demo.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8VR5QUw8ieGh"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from typing import Dict, Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "from collections import deque, OrderedDict\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from gym import Env\n",
    "import gymnasium as gym\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "\n",
    "from doom_arena import VizdoomMPEnv\n",
    "from doom_arena.reward import VizDoomReward\n",
    "from doom_arena.render import render_episode\n",
    "from IPython.display import HTML\n",
    "\n",
    "from vizdoom import ScreenFormat\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmNDlnmfzP62"
   },
   "outputs": [],
   "source": [
    "USE_GRAYSCALE = False # ← flip to False for RGB\n",
    "\n",
    "PLAYER_CONFIG = {\n",
    "    \"n_stack_frames\": 1,\n",
    "    \"extra_state\": [\"depth\"],\n",
    "    \"hud\": \"none\",\n",
    "    \"crosshair\": True,\n",
    "    \"screen_format\": ScreenFormat.CRCGCB if not USE_GRAYSCALE else ScreenFormat.GRAY8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLoMT0EMzP63"
   },
   "source": [
    "## Reward function\n",
    "In this task, you will define a reward function to guide the agent's learning. The function is called at every step and receives the current and previous game variables (e.g., number of frags, hits taken, health).\n",
    "\n",
    "Your goal is to combine these into a meaningful reward, encouraging desirable behavior, such as:\n",
    "\n",
    "- Rewarding frags (enemy kills)\n",
    "\n",
    "- Rewarding accuracy (hitting enemies)\n",
    "\n",
    "- Penalizing damage taken\n",
    "\n",
    "- (Optional) Encouraging survival, ammo efficiency, etc.\n",
    "\n",
    "You can return multiple reward components, which are summed during training. Consider the class below as a great starting point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A3vdawvQzP62"
   },
   "outputs": [],
   "source": [
    "# TODO: environment training paramters\n",
    "N_STACK_FRAMES = 4\n",
    "NUM_BOTS = 4\n",
    "EPISODE_TIMEOUT = 1000\n",
    "# TODO: model hyperparams\n",
    "GAMMA = 0.95\n",
    "EPISODES = 1000 \n",
    "BATCH_SIZE = 256\n",
    "REPLAY_BUFFER_SIZE = 100000\n",
    "LEARNING_RATE = 5e-4\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 0.999\n",
    "FEATURES_DIM = 512\n",
    "#N_EPOCHS = 50 # Not used with stable_baseline3\n",
    "TOTAL_TIMESTEPS = 1000000\n",
    "EXPLORATION_FRACTION = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Gq1xlneLzP63"
   },
   "outputs": [],
   "source": [
    "class YourReward(VizDoomReward):\n",
    "    def __init__(self, num_players: int):\n",
    "        super().__init__(num_players)\n",
    "        # Initialize a list to store the last computed reward components for each player\n",
    "        self.last_rewards = [None] * num_players\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        vizdoom_reward: float,\n",
    "        game_var: Dict[str, float],\n",
    "        game_var_old: Dict[str, float],\n",
    "        player_id: int,\n",
    "    ) -> Tuple[float, ...]:\n",
    "        \"\"\"\n",
    "        Custom reward function for training and evaluation:\n",
    "        * +0.01  for each damage point dealt (rwd_damage)\n",
    "        * -0.1   for each missed shot (rwd_missed)\n",
    "        * +1.0   for each new frag (rwd_frag)\n",
    "        * +0.02  for each health point gained (rwd_health_pickup)\n",
    "        * +0.001 for surviving each step (rwd_survival)\n",
    "        * -0.5   if the player dies (rwd_dead)\n",
    "        * -0.01  for shooting without dealing damage (rwd_spam_penalty)\n",
    "        * +0.00005 for moving, -0.0025 for staying still (rwd_movement)\n",
    "        \"\"\"\n",
    "        # Increment internal step counter\n",
    "        self._step += 1\n",
    "        # Ignore vizdoom_reward and player_id for internal use\n",
    "        _ = vizdoom_reward, player_id\n",
    "\n",
    "        # Calculate reward for damage dealt\n",
    "        damage_done = game_var[\"DAMAGECOUNT\"] - game_var_old[\"DAMAGECOUNT\"]\n",
    "        rwd_damage = 0.01 * damage_done\n",
    "\n",
    "        # Calculate reward for achieving a frag\n",
    "        rwd_frag = 1.0 * (game_var[\"FRAGCOUNT\"] - game_var_old[\"FRAGCOUNT\"])\n",
    "\n",
    "        # Calculate shots fired and missed shots\n",
    "        ammo_delta = game_var_old[\"SELECTED_WEAPON_AMMO\"] - game_var[\"SELECTED_WEAPON_AMMO\"]\n",
    "        if ammo_delta > 0:\n",
    "            shots_fired = ammo_delta\n",
    "            hits = game_var[\"HITCOUNT\"] - game_var_old[\"HITCOUNT\"]\n",
    "            missed_shots = max(0, shots_fired - hits)\n",
    "            rwd_missed = -0.1 * missed_shots\n",
    "        else:\n",
    "            rwd_missed = 0\n",
    "\n",
    "        # Calculate survival reward and death penalty\n",
    "        rwd_survival = 0.001\n",
    "        rwd_dead = -0.5 if game_var[\"DEAD\"] == 1 else 0.0\n",
    "\n",
    "        # Calculate penalty for spamming shots without dealing damage\n",
    "        rwd_spam_penalty = -0.01 if ammo_delta > 0 and damage_done <= 0 else 0.0\n",
    "\n",
    "        # Calculate reward for gaining health\n",
    "        health_delta = game_var[\"HEALTH\"] - game_var_old[\"HEALTH\"]\n",
    "        health_gained = max(0, health_delta)\n",
    "        rwd_health_pickup = 0.02 * health_gained\n",
    "\n",
    "        # Calculate reward for movement\n",
    "        position_changed = (game_var[\"POSITION_X\"] != game_var_old[\"POSITION_X\"]) or (game_var[\"POSITION_Y\"] != game_var_old[\"POSITION_Y\"])\n",
    "        rwd_movement = 0.00005 if position_changed else -0.0025\n",
    "\n",
    "        # Store the computed reward components for the current player\n",
    "        rewards = (rwd_damage, rwd_frag, rwd_missed, rwd_survival, rwd_dead, rwd_spam_penalty, rwd_health_pickup, rwd_movement)\n",
    "        self.last_rewards[player_id] = rewards\n",
    "        \n",
    "        # Return the tuple of reward components\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzTS4m2VzP63",
    "outputId": "826001b6-834a-4a49-e8c8-2304a24b0f32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host 43263\n",
      "Player 43263\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "DTYPE = torch.float32\n",
    "\n",
    "reward_fn = YourReward(num_players=1)\n",
    "\n",
    "from vizdoom import Button, ScreenFormat\n",
    "\n",
    "env = VizdoomMPEnv(\n",
    "    num_players=1,\n",
    "    num_bots=NUM_BOTS,\n",
    "    bot_skill=0,\n",
    "    doom_map=\"ROOM\",\n",
    "    extra_state=PLAYER_CONFIG[\"extra_state\"],\n",
    "    episode_timeout=EPISODE_TIMEOUT,\n",
    "    n_stack_frames=PLAYER_CONFIG[\"n_stack_frames\"],\n",
    "    crosshair=PLAYER_CONFIG[\"crosshair\"],\n",
    "    hud=PLAYER_CONFIG[\"hud\"],\n",
    "    reward_fn=None,\n",
    "    available_buttons=[Button.ATTACK, Button.TURN_LEFT, Button.TURN_RIGHT],\n",
    "    frame_skip=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkuIefaSzP64"
   },
   "source": [
    "## Agent\n",
    "\n",
    "Implement **your own agent** in the code cell that follows.\n",
    "\n",
    "* In `agents/dqn.py` and `agents/ppo.py` you’ll find very small **skeletons**—they compile but are meant only as reference or quick tests.  \n",
    "  Feel free to open them, borrow ideas, extend them, or ignore them entirely.\n",
    "* The notebook does **not** import those files automatically; whatever class you define in the next cell is the one that will be trained.\n",
    "* You may keep the DQN interface, switch to PPO, or try something else.\n",
    "* Tweak any hyper-parameters (`PLAYER_CONFIG`, ε-schedule, optimiser, etc.) and document what you tried.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vmEX6eV98Xpu"
   },
   "outputs": [],
   "source": [
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        assert isinstance(observation_space, gym.spaces.Box), \"Observation space must be a Box\"\n",
    "\n",
    "        # Annahme: Die Kanäle sind wie folgt geordnet: Bildschirm (3), Tiefe (1), Labels (1)\n",
    "        c = observation_space.shape[2]\n",
    "        screen_ch = 3\n",
    "        depth_ch = 1\n",
    "        labels_ch = 1\n",
    "        assert screen_ch + depth_ch + labels_ch == c, \"Kanal-Mismatch\"\n",
    "\n",
    "        # Definiere CNNs für jeden Teil\n",
    "        self.cnn_screen = nn.Sequential(\n",
    "            nn.Conv2d(screen_ch, 16, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=0),         \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),   \n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.cnn_depth = nn.Sequential(\n",
    "            nn.Conv2d(depth_ch, 16, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.cnn_labels = nn.Sequential(\n",
    "            nn.Conv2d(labels_ch, 16, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Berechne die flachen Größen\n",
    "        with torch.no_grad():\n",
    "            sample = torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            sample = rearrange(sample, 'n h w c -> n c h w')\n",
    "            n_flatten_screen = self.cnn_screen(sample[:, :screen_ch, :, :]).shape[1]\n",
    "            n_flatten_depth = self.cnn_depth(sample[:, screen_ch:screen_ch+depth_ch, :, :]).shape[1]\n",
    "            n_flatten_labels = self.cnn_labels(sample[:, screen_ch+depth_ch:, :, :]).shape[1]\n",
    "\n",
    "        total_flatten = n_flatten_screen + n_flatten_depth + n_flatten_labels\n",
    "        self.linear = nn.Sequential(nn.Linear(total_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        # observations shape: (N, H, W, C)\n",
    "        observations = rearrange(observations, 'n h w c -> n c h w')\n",
    "        screen = observations[:, :3, :, :]\n",
    "        depth = observations[:, 3:4, :, :]\n",
    "        labels = observations[:, 4:5, :, :]\n",
    "        features_screen = self.cnn_screen(screen)\n",
    "        features_depth = self.cnn_depth(depth)\n",
    "        features_labels = self.cnn_labels(labels)\n",
    "        combined = torch.cat((features_screen, features_depth, features_labels), dim=1)\n",
    "        return self.linear(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7dWjwTdzP65",
    "outputId": "598d5e62-2050-4063-9884-27fe655ed84a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Initialise your networks and training utilities\n",
    "# ================================================================\n",
    "\n",
    "# main Q-network\n",
    "in_channels = env.observation_space.shape[0]   # 1 if grayscale, else 3/4\n",
    "#model = DQN(\n",
    "#    input_dim    = in_channels,\n",
    "#    action_space = env.action_space.n,\n",
    "#    hidden       = 64,   # change or ignore\n",
    "#).to(device, dtype=DTYPE)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=FEATURES_DIM),\n",
    ")\n",
    "\n",
    "model = DQN(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    learning_rate=0.0001,          \n",
    "    buffer_size=10000,            \n",
    "    batch_size=32,                 \n",
    "    gamma=0.99,                    \n",
    "    exploration_fraction=0.1,     \n",
    "    exploration_initial_eps=1.0,   \n",
    "    exploration_final_eps=0.02,   \n",
    "    verbose=1,\n",
    "    policy_kwargs=policy_kwargs,\n",
    ")\n",
    "\n",
    "# TODO ------------------------------------------------------------\n",
    "# 1. Create a target network (hard-copy or EMA)\n",
    "# 2. Choose an optimiser + learning-rate schedule\n",
    "# 3. Instantiate a replay buffer and set the initial epsilon value\n",
    "#\n",
    "# Hints:\n",
    "#   model_tgt  = deepcopy(model).to(device)y\n",
    "#   optimiser  = torch.optim.Adam(...)\n",
    "#   scheduler  = torch.optim.lr_scheduler.ExponentialLR(...)\n",
    "#   replay_buf = collections.deque(maxlen=...)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "#model_tgt = deepcopy(model).to(device, dtype=DTYPE)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "#replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)\n",
    "#epsilon = EPSILON_START\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "izoQ5C4WPN5y"
   },
   "outputs": [],
   "source": [
    "class EpisodeCallback(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super(EpisodeCallback, self).__init__()\n",
    "        self.episode_reward = 0\n",
    "        self.episode_num = 0\n",
    "        self.episode_rwd_components = {\n",
    "            \"hit\": 0.0,\n",
    "            \"hit_taken\": 0.0,\n",
    "            \"frag\": 0.0,\n",
    "            \"missed\": 0.0,\n",
    "            \"survival\": 0.0,\n",
    "            \"dead\": 0.0,\n",
    "            \"spam_penalty\": 0.0,\n",
    "            \"health\": 0.0\n",
    "        }\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Accumulate the total reward for the current step\n",
    "        self.episode_reward += self.locals['rewards'][0]\n",
    "\n",
    "        # Access the actual environment (VizdoomMPEnv) through DummyVecEnv and Monitor\n",
    "        monitor_env = self.locals['env'].envs[0]  # This is the Monitor wrapper\n",
    "        actual_env = monitor_env.env              # This is the VizdoomMPEnv\n",
    "        last_rewards = actual_env.reward_fn.last_rewards[0]\n",
    "\n",
    "        # Update reward components if rewards are available\n",
    "        if last_rewards is not None:\n",
    "            self.episode_rwd_components[\"hit\"] += last_rewards[0]\n",
    "            self.episode_rwd_components[\"hit_taken\"] += last_rewards[1]\n",
    "            self.episode_rwd_components[\"frag\"] += last_rewards[2]\n",
    "            self.episode_rwd_components[\"missed\"] += last_rewards[3]\n",
    "            self.episode_rwd_components[\"survival\"] += last_rewards[4]\n",
    "            self.episode_rwd_components[\"dead\"] += last_rewards[5]\n",
    "            self.episode_rwd_components[\"spam_penalty\"] += last_rewards[6]\n",
    "            self.episode_rwd_components[\"health\"] += last_rewards[7]\n",
    "\n",
    "        # Log data if the episode ends\n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_num += 1\n",
    "            wandb.log({\n",
    "                \"episode\": self.episode_num,\n",
    "                \"return\": self.episode_reward,\n",
    "                \"rwd_hit\": self.episode_rwd_components[\"hit\"],\n",
    "                \"rwd_hit_taken\": self.episode_rwd_components[\"hit_taken\"],\n",
    "                \"rwd_frag\": self.episode_rwd_components[\"frag\"],\n",
    "                \"rwd_missed\": self.episode_rwd_components[\"missed\"],\n",
    "                \"rwd_survival\": self.episode_rwd_components[\"survival\"],\n",
    "                \"rwd_dead\": self.episode_rwd_components[\"dead\"],\n",
    "                \"rwd_spam_penalty\": self.episode_rwd_components[\"spam_penalty\"],\n",
    "                \"rwd_health\": self.episode_rwd_components[\"health\"],\n",
    "            })\n",
    "            # Reset accumulators\n",
    "            self.episode_reward = 0\n",
    "            for key in self.episode_rwd_components:\n",
    "                self.episode_rwd_components[key] = 0.0\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mihj8Q5xzP65"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815,
     "referenced_widgets": [
      "3c31dc8a260d4df1bb95e13f7eb51a24",
      "6598c36296d741b9ba9c4027eb3e326d"
     ]
    },
    "id": "W5V4XMeIk5Uv",
    "outputId": "4494dc46-bb92-4e0c-a0f7-3fe97963eb06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoerenseitz\u001b[0m (\u001b[33msoerenseitz-university-of-vienna\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/jku.wad/wandb/run-20250619_090428-vncle3p5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/soerenseitz-university-of-vienna/doom-rl/runs/vncle3p5' target=\"_blank\">denim-breeze-88</a></strong> to <a href='https://wandb.ai/soerenseitz-university-of-vienna/doom-rl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/soerenseitz-university-of-vienna/doom-rl' target=\"_blank\">https://wandb.ai/soerenseitz-university-of-vienna/doom-rl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/soerenseitz-university-of-vienna/doom-rl/runs/vncle3p5' target=\"_blank\">https://wandb.ai/soerenseitz-university-of-vienna/doom-rl/runs/vncle3p5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc16e71fe5140c89a95e2b1d18ad13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.54    |\n",
      "|    exploration_rate | 0.982    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 4000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.00022  |\n",
      "|    n_updates        | 974      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.47    |\n",
      "|    exploration_rate | 0.964    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 147      |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000193 |\n",
      "|    n_updates        | 1974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.44    |\n",
      "|    exploration_rate | 0.946    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 220      |\n",
      "|    total_timesteps  | 12000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000177 |\n",
      "|    n_updates        | 2974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.46    |\n",
      "|    exploration_rate | 0.928    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 297      |\n",
      "|    total_timesteps  | 16000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000114 |\n",
      "|    n_updates        | 3974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -8.22    |\n",
      "|    exploration_rate | 0.91     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 375      |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000126 |\n",
      "|    n_updates        | 4974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -9.37    |\n",
      "|    exploration_rate | 0.892    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 450      |\n",
      "|    total_timesteps  | 24000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000309 |\n",
      "|    n_updates        | 5974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -10.5    |\n",
      "|    exploration_rate | 0.874    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 525      |\n",
      "|    total_timesteps  | 28000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000189 |\n",
      "|    n_updates        | 6974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -12.6    |\n",
      "|    exploration_rate | 0.856    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 601      |\n",
      "|    total_timesteps  | 32000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000181 |\n",
      "|    n_updates        | 7974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -14.1    |\n",
      "|    exploration_rate | 0.838    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 712      |\n",
      "|    total_timesteps  | 36000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000164 |\n",
      "|    n_updates        | 8974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -13.1    |\n",
      "|    exploration_rate | 0.82     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 789      |\n",
      "|    total_timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000199 |\n",
      "|    n_updates        | 9974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -12.2    |\n",
      "|    exploration_rate | 0.802    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 864      |\n",
      "|    total_timesteps  | 44000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000207 |\n",
      "|    n_updates        | 10974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.5    |\n",
      "|    exploration_rate | 0.784    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 939      |\n",
      "|    total_timesteps  | 48000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.00046  |\n",
      "|    n_updates        | 11974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -10.9    |\n",
      "|    exploration_rate | 0.766    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 1015     |\n",
      "|    total_timesteps  | 52000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000593 |\n",
      "|    n_updates        | 12974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -10.3    |\n",
      "|    exploration_rate | 0.748    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 1093     |\n",
      "|    total_timesteps  | 56000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000167 |\n",
      "|    n_updates        | 13974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -9.86    |\n",
      "|    exploration_rate | 0.73     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 49       |\n",
      "|    time_elapsed     | 1202     |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000131 |\n",
      "|    n_updates        | 14974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -9.44    |\n",
      "|    exploration_rate | 0.712    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 3073     |\n",
      "|    total_timesteps  | 64000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000108 |\n",
      "|    n_updates        | 15974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -9.07    |\n",
      "|    exploration_rate | 0.694    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 17       |\n",
      "|    time_elapsed     | 3901     |\n",
      "|    total_timesteps  | 68000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000331 |\n",
      "|    n_updates        | 16974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -8.82    |\n",
      "|    exploration_rate | 0.676    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 5679     |\n",
      "|    total_timesteps  | 72000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000237 |\n",
      "|    n_updates        | 17974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -8.53    |\n",
      "|    exploration_rate | 0.658    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 5806     |\n",
      "|    total_timesteps  | 76000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 8.57e-05 |\n",
      "|    n_updates        | 18974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -9.44    |\n",
      "|    exploration_rate | 0.64     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 5898     |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000601 |\n",
      "|    n_updates        | 19974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -9.16    |\n",
      "|    exploration_rate | 0.622    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 6012     |\n",
      "|    total_timesteps  | 84000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000197 |\n",
      "|    n_updates        | 20974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -8.89    |\n",
      "|    exploration_rate | 0.604    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 6134     |\n",
      "|    total_timesteps  | 88000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000173 |\n",
      "|    n_updates        | 21974    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------  TRAINING LOOP  ----------------------\n",
    "# Feel free to change EVERYTHING below:\n",
    "#   • choose your own reward function\n",
    "#   • track different episode statistics in `ep_metrics`\n",
    "#   • switch optimiser, scheduler, update rules, etc.\n",
    "run = wandb.init(project=\"doom-rl\", entity=\"soerenseitz-university-of-vienna\", config={\n",
    "    \"gamma\": GAMMA,\n",
    "    \"episodes\": EPISODES,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"replay_buffer_size\": REPLAY_BUFFER_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epsilon_start\": EPSILON_START,\n",
    "    \"epsilon_end\": EPSILON_END,\n",
    "    \"epsilon_decay\": EPSILON_DECAY,\n",
    "    \"num_bots\": NUM_BOTS,\n",
    "    \"episode_timeout\": EPISODE_TIMEOUT,\n",
    "    \"use_grayscale\": USE_GRAYSCALE,\n",
    "    \"extra_state\": PLAYER_CONFIG[\"extra_state\"],\n",
    "    \"hud\": PLAYER_CONFIG[\"hud\"],\n",
    "    \"crosshair\": PLAYER_CONFIG[\"crosshair\"],\n",
    "    \"screen_format\": PLAYER_CONFIG[\"screen_format\"].name,\n",
    "    \"doom_map\": \"ROOM\",\n",
    "})\n",
    "callback = EpisodeCallback()\n",
    "\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callback, progress_bar=True)\n",
    "final_model = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpBx9O9yzP66"
   },
   "source": [
    "## Dump to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNkgBbUSzP66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy input shape: torch.Size([1, 128, 128, 5])\n",
      "Best network exported to model.onnx\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁</td></tr><tr><td>return</td><td>▁</td></tr><tr><td>rwd_dead</td><td>▁</td></tr><tr><td>rwd_frag</td><td>▁</td></tr><tr><td>rwd_health</td><td>▁</td></tr><tr><td>rwd_hit</td><td>▁</td></tr><tr><td>rwd_hit_taken</td><td>▁</td></tr><tr><td>rwd_missed</td><td>▁</td></tr><tr><td>rwd_spam_penalty</td><td>▁</td></tr><tr><td>rwd_survival</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>1</td></tr><tr><td>return</td><td>-1.4305</td></tr><tr><td>rwd_dead</td><td>-0.2</td></tr><tr><td>rwd_frag</td><td>-2.0</td></tr><tr><td>rwd_health</td><td>-0.2305</td></tr><tr><td>rwd_hit</td><td>0</td></tr><tr><td>rwd_hit_taken</td><td>0</td></tr><tr><td>rwd_missed</td><td>1.0</td></tr><tr><td>rwd_spam_penalty</td><td>0</td></tr><tr><td>rwd_survival</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swept-river-83</strong> at: <a href='https://wandb.ai/soerenseitz-university-of-vienna/doom-rl/runs/7cy8h2a2' target=\"_blank\">https://wandb.ai/soerenseitz-university-of-vienna/doom-rl/runs/7cy8h2a2</a><br> View project at: <a href='https://wandb.ai/soerenseitz-university-of-vienna/doom-rl' target=\"_blank\">https://wandb.ai/soerenseitz-university-of-vienna/doom-rl</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250618_100101-7cy8h2a2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import onnx\n",
    "import json\n",
    "\n",
    "def onnx_dump(env, model, config, run, filename_prefix=\"model\"):\n",
    "    # Create dummy input on CPU\n",
    "    dummy_input = torch.randn(1, *env.observation_space.shape).float().to('cpu')\n",
    "    print(\"Dummy input shape:\", dummy_input.shape)  # Debug output\n",
    "    \n",
    "    # Ensure policy network is on CPU\n",
    "    policy_net = model.policy.to('cpu')\n",
    "    \n",
    "    # Generate unique filename using wandb run ID\n",
    "    run_id = run.id\n",
    "    filename = f\"{filename_prefix}_{run_id}.onnx\"\n",
    "    \n",
    "    # Export the model to ONNX\n",
    "    torch.onnx.export(\n",
    "        policy_net,\n",
    "        args=dummy_input,\n",
    "        f=filename,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    )\n",
    "    \n",
    "    # Add metadata to the ONNX model\n",
    "    onnx_model = onnx.load(filename)\n",
    "    meta = onnx_model.metadata_props.add()\n",
    "    meta.key = \"config\"\n",
    "    meta.value = json.dumps(config)\n",
    "    onnx.save(onnx_model, filename)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Usage\n",
    "export_config = {\n",
    "    **{k: str(v) if isinstance(v, ScreenFormat) else v for k, v in PLAYER_CONFIG.items()},\n",
    "    \"algo_type\": \"Q\",\n",
    "}\n",
    "\n",
    "# Assuming 'run' is the wandb run object\n",
    "filename = onnx_dump(env, final_model, export_config, run, filename_prefix=\"model\")\n",
    "print(f\"Best network exported to {filename}\")\n",
    "\n",
    "# Upload to wandb\n",
    "artifact = wandb.Artifact('model', type='model')\n",
    "artifact.add_file(filename)\n",
    "run.log_artifact(artifact)\n",
    "artifact.wait()\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rZCsfQHFMu-"
   },
   "source": [
    "### Evaluation and Visualization\n",
    "\n",
    "In this final section, you can evaluate your trained agent, inspect its performance visually, and analyze reward components over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARC2K2k686nu"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 📈  Reward-plot helper  (feel free to edit / extend)\n",
    "# ---------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_reward_components(reward_log, smooth_window: int = 5):\n",
    "    \"\"\"\n",
    "    Plot raw and smoothed episode-level reward components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reward_log : list[dict]\n",
    "        Append a dict for each episode, e.g. {\"frag\": …, \"hit\": …, \"hittaken\": …}\n",
    "    smooth_window : int\n",
    "        Rolling-mean window size for the smoothed curve.\n",
    "    \"\"\"\n",
    "    if not reward_log:\n",
    "        print(\"reward_log is empty – nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(reward_log)\n",
    "    df_smooth = df.rolling(window=smooth_window, min_periods=1).mean()\n",
    "\n",
    "    # raw\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for col in df.columns:\n",
    "        plt.plot(df.index, df[col], label=col)\n",
    "    plt.title(\"Raw episode reward components\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # smoothed\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for col in df.columns:\n",
    "        plt.plot(df.index, df_smooth[col], label=f\"{col} (avg)\")\n",
    "    plt.title(f\"Smoothed (window={smooth_window})\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 🔍  Hint for replay visualisation:\n",
    "# ----------------------------------------------------------------\n",
    "# env.enable_replay()\n",
    "# ... run an evaluation episode ...\n",
    "# env.disable_replay()\n",
    "# replays = env.get_player_replays()\n",
    "#\n",
    "# from doom_arena.render import render_episode\n",
    "# from IPython.display import HTML\n",
    "# HTML(render_episode(replays, subsample=5).to_html5_video())\n",
    "#\n",
    "# Feel free to adapt or write your own GIF/MP4 export.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3c31dc8a260d4df1bb95e13f7eb51a24": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_6598c36296d741b9ba9c4027eb3e326d",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   1%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">10,796/1,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:03:40</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">5:39:14</span> , <span style=\"color: #800000; text-decoration-color: #800000\">49 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[35m   1%\u001b[0m \u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10,796/1,000,000 \u001b[0m [ \u001b[33m0:03:40\u001b[0m < \u001b[36m5:39:14\u001b[0m , \u001b[31m49 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "6598c36296d741b9ba9c4027eb3e326d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
